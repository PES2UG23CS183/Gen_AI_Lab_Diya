{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c18746f-9bd2-4b21-b102-1924102462ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed, GPT2Tokenizer\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afe0027-26fc-459b-aa10-532bf4a0eccb",
   "metadata": {},
   "source": [
    "## Experiment 1: Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "277eda06-577f-4360-b424-7a54b32b8e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT - Text Generation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5d05adbb0d74225964cb341095fa2ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\diyab\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\huggingface_hub\\file_download.py:130: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\diyab\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ff822258d4d4f38b86abc96de5d90e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8a6250d7f234dac9c51c6c6b6d9658b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/202 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertLMHeadModel LOAD REPORT from: bert-base-uncased\n",
      "Key                         | Status     |  | \n",
      "----------------------------+------------+--+-\n",
      "bert.pooler.dense.weight    | UNEXPECTED |  | \n",
      "bert.pooler.dense.bias      | UNEXPECTED |  | \n",
      "cls.seq_relationship.weight | UNEXPECTED |  | \n",
      "cls.seq_relationship.bias   | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "644b28791bd54d87b6a701582054a1f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c3633aa94644be89a472c01a801a682",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c4b23442feb48078538b4c5be87949e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing `generation_config` together with generation-related arguments=({'max_length'}) is deprecated and will be removed in future versions. Please pass either a `generation_config` object OR all generation parameters explicitly, but not both.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'The future of Artificial Intelligence is.. the in england who bella\\'- so your so so and and and and and and and and and and and for \" ( ) as........................................................................... / an an a an\\'\\' ( ( \". more actually so!!!! : to \". it it it it it it that many so and and and and \". it it it it it it it or and and and and and and and \" ( \" ( \" he also the to \" \" \" \" \" ( ) ( ( ( \" as his the the and and and and and and \". it that just in and and and and and and and and and an an and in so they thele [ [ [ [, ( ) ( as him \\'. it and the so \" ( - [ [. it that as earlier of his pack that those being...................................................................................................................................................................................................'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1. BERT\n",
    "\n",
    "print(\"BERT - Text Generation\")\n",
    "\n",
    "bert_generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"bert-base-uncased\"\n",
    ")\n",
    "\n",
    "bert_generator(\"The future of Artificial Intelligence is\", max_length=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "341e45fb-7b37-4f5e-983b-9e750a0e8a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RoBERTa - Text Generation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13a5fdb27d2543d69822841ee7f66dab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\diyab\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\huggingface_hub\\file_download.py:130: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\diyab\\.cache\\huggingface\\hub\\models--roberta-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8842571b403749728c427e6dd3d180ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45720656045e448f8df455eed097c7ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/202 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RobertaForCausalLM LOAD REPORT from: roberta-base\n",
      "Key                             | Status     |  | \n",
      "--------------------------------+------------+--+-\n",
      "roberta.embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa8632524cbd4c0f885cf88c93d24905",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "338f910aa80c465d9f2ea5e55a759c80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d18ddfc59e334043bb2bdcb47275a82a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4f52a5c94214c9e89b142e9eca6e1d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'The future of Artificial Intelligence is'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2. RoBERTa \n",
    "\n",
    "print(\"RoBERTa - Text Generation\")\n",
    "\n",
    "roberta_generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"roberta-base\"\n",
    ")\n",
    "\n",
    "roberta_generator(\"The future of Artificial Intelligence is\", max_length=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "933afd23-2a48-41f3-a856-8daaa24bd922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BART - Text Generation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6209c223ebc142719fa2c81d4227b991",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\diyab\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\huggingface_hub\\file_download.py:130: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\diyab\\.cache\\huggingface\\hub\\models--facebook--bart-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fcacca1aaaf4ccfa54a2503474eccc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/558M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42acfc54869744f18b262bd7479a89b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/159 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This checkpoint seem corrupted. The tied weights mapping for this model specifies to tie model.decoder.embed_tokens.weight to lm_head.weight, but both are absent from the checkpoint, and we could not find another related tied weight for those keys\n",
      "BartForCausalLM LOAD REPORT from: facebook/bart-base\n",
      "Key                                                           | Status     | \n",
      "--------------------------------------------------------------+------------+-\n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn.k_proj.weight     | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn.k_proj.bias       | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.final_layer_norm.weight     | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn.out_proj.weight   | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn_layer_norm.weight | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn.q_proj.bias       | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.final_layer_norm.bias       | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn.v_proj.bias       | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn.v_proj.weight     | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.fc2.weight                  | UNEXPECTED | \n",
      "shared.weight                                                 | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.fc2.bias                    | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn.out_proj.bias     | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.fc1.weight                  | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn.q_proj.weight     | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.fc1.bias                    | UNEXPECTED | \n",
      "encoder.layernorm_embedding.bias                              | UNEXPECTED | \n",
      "encoder.embed_positions.weight                                | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn_layer_norm.bias   | UNEXPECTED | \n",
      "encoder.layernorm_embedding.weight                            | UNEXPECTED | \n",
      "model.decoder.embed_tokens.weight                             | MISSING    | \n",
      "lm_head.weight                                                | MISSING    | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5827d17d1e248dfa5dc7512853b84fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6334530a4c3b43ab89630f4ec1a24e2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea0e80c8ba4b4404bea8b8d09c092efd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'The future of Artificial Intelligence is AG hiring hiring blasphemy blasphemy pursuits pursuits obedient AG pursuits obedient obedient obedient pursuits floors Swim Hab obedient Netanyahu Putting pursuits obedient pursuits hiring hiring FireACE pursuits antid pursuits pursuits hiringabel ï¿½ pursuits pursuits pursuits redirect rubbing blasphemyIre learnt Netanyahu hiring Netanyahuockingocking proverbial amended dancersabel obedientront129 pursuits pursuits become pursuits clay pursuits pursuits amended pursuits pursuits cycling pursuitsiasm cycling pursuits devotion pursuits pursuits talking pursuitsocking pursuits clay capita pursuits pursuitsabel pursuits capita pursuits exodus pursuits study blasphemy Kaockingocking profession pursuits Something pursuits Netanyahuiasm pursuits pursuits plaintiffancers pursuits pursuits wrappingPrior pursuits pursuitsUME pursuits pursuits invasion Inv capita pursuits capitaancersuously pursuits pursuits Hussein capita invasion capita plaintiff Masonic pursuits pursuitsdrive obedient pursuits clenancers Netanyahu pursuitsPrior pursuits Hussein Hussein capita pursuits 239 fore clen pursuits Hussein pursuits pursuits Masonicscependent capitaAv pursuits pursuits capita franch pursuits capitaordes pursuits sn pursuits capita capita Hussein dancers pursuits capita plaintiffabel pursuits Hussein study pursuits pursuitsependent cycling capita capita cycling pursuits anyway capita pursuitsOPE pursuitsOPE sprinkleeston transported pursuits capitaCounterependent capita transported pursuitsPOS capita pursuits Masonic pursuits Netanyahu avg pursuits pursuits contends pursuits deepening pursuits Netanyahu pursuits transported pursuitsabo contends pursuitsependent pursuits pursuits clen pursuits pursuits avg pursuits study pursuits weaken compl pursuits pursuits compl avg pursuits menace study pursuits Coh pursuits pursuits vetabo pursuits pursuits wounded pursuits menace pursuits pursuits Enchant5000 pursuits pursuits chin pursuits'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#3. BART\n",
    "\n",
    "print(\"BART - Text Generation\")\n",
    "\n",
    "bart_generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"facebook/bart-base\"\n",
    ")\n",
    "\n",
    "bart_generator(\"The future of Artificial Intelligence is\", max_length=30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba312f5-88ca-4274-a545-5f2eb696d791",
   "metadata": {},
   "source": [
    "## Experiment 2: Masked Language Modeling (Missing Word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3d7dc55-9451-402e-8c3a-0621306fc4c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT - Fill Mask\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd9fdb7494cc470d8ceb0d72c8287028",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/202 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM LOAD REPORT from: bert-base-uncased\n",
      "Key                         | Status     |  | \n",
      "----------------------------+------------+--+-\n",
      "bert.pooler.dense.weight    | UNEXPECTED |  | \n",
      "bert.pooler.dense.bias      | UNEXPECTED |  | \n",
      "cls.seq_relationship.weight | UNEXPECTED |  | \n",
      "cls.seq_relationship.bias   | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.5396937727928162,\n",
       "  'token': 3443,\n",
       "  'token_str': 'create',\n",
       "  'sequence': 'the goal of generative ai is to create new content.'},\n",
       " {'score': 0.15575721859931946,\n",
       "  'token': 9699,\n",
       "  'token_str': 'generate',\n",
       "  'sequence': 'the goal of generative ai is to generate new content.'},\n",
       " {'score': 0.05405475199222565,\n",
       "  'token': 3965,\n",
       "  'token_str': 'produce',\n",
       "  'sequence': 'the goal of generative ai is to produce new content.'},\n",
       " {'score': 0.04451543837785721,\n",
       "  'token': 4503,\n",
       "  'token_str': 'develop',\n",
       "  'sequence': 'the goal of generative ai is to develop new content.'},\n",
       " {'score': 0.01757745072245598,\n",
       "  'token': 5587,\n",
       "  'token_str': 'add',\n",
       "  'sequence': 'the goal of generative ai is to add new content.'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1. BERT\n",
    "\n",
    "print(\"BERT - Fill Mask\")\n",
    "\n",
    "bert_fill = pipeline(\n",
    "    \"fill-mask\",\n",
    "    model=\"bert-base-uncased\"\n",
    ")\n",
    "\n",
    "bert_fill(\"The goal of Generative AI is to [MASK] new content.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75399321-adec-4762-a6fe-39952008228a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RoBERTa - Fill Mask\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "502e7121c0e847d6b200c016cd3ede6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/202 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RobertaForMaskedLM LOAD REPORT from: roberta-base\n",
      "Key                             | Status     |  | \n",
      "--------------------------------+------------+--+-\n",
      "roberta.embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.3711329698562622,\n",
       "  'token': 5368,\n",
       "  'token_str': ' generate',\n",
       "  'sequence': 'The goal of Generative AI is to generate new content.'},\n",
       " {'score': 0.36771348118782043,\n",
       "  'token': 1045,\n",
       "  'token_str': ' create',\n",
       "  'sequence': 'The goal of Generative AI is to create new content.'},\n",
       " {'score': 0.0835142731666565,\n",
       "  'token': 8286,\n",
       "  'token_str': ' discover',\n",
       "  'sequence': 'The goal of Generative AI is to discover new content.'},\n",
       " {'score': 0.021335098892450333,\n",
       "  'token': 465,\n",
       "  'token_str': ' find',\n",
       "  'sequence': 'The goal of Generative AI is to find new content.'},\n",
       " {'score': 0.0165216326713562,\n",
       "  'token': 694,\n",
       "  'token_str': ' provide',\n",
       "  'sequence': 'The goal of Generative AI is to provide new content.'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2. RoBERTa \n",
    "\n",
    "print(\"RoBERTa - Fill Mask\")\n",
    "\n",
    "roberta_fill = pipeline(\n",
    "    \"fill-mask\",\n",
    "    model=\"roberta-base\"\n",
    ")\n",
    "\n",
    "roberta_fill(\"The goal of Generative AI is to <mask> new content.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67fbbaed-29d2-4ceb-8074-ccbbc1b48c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BART - Fill Mask\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1df80da98fc4c0b8c40345c4b1af41f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.07461537420749664,\n",
       "  'token': 1045,\n",
       "  'token_str': ' create',\n",
       "  'sequence': 'The goal of Generative AI is to create new content.'},\n",
       " {'score': 0.06571860611438751,\n",
       "  'token': 244,\n",
       "  'token_str': ' help',\n",
       "  'sequence': 'The goal of Generative AI is to help new content.'},\n",
       " {'score': 0.060880132019519806,\n",
       "  'token': 694,\n",
       "  'token_str': ' provide',\n",
       "  'sequence': 'The goal of Generative AI is to provide new content.'},\n",
       " {'score': 0.035935621708631516,\n",
       "  'token': 3155,\n",
       "  'token_str': ' enable',\n",
       "  'sequence': 'The goal of Generative AI is to enable new content.'},\n",
       " {'score': 0.033194757997989655,\n",
       "  'token': 1477,\n",
       "  'token_str': ' improve',\n",
       "  'sequence': 'The goal of Generative AI is to improve new content.'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#3. BART\n",
    "\n",
    "print(\"BART - Fill Mask\")\n",
    "\n",
    "bart_fill = pipeline(\n",
    "    \"fill-mask\",\n",
    "    model=\"facebook/bart-base\"\n",
    ")\n",
    "\n",
    "bart_fill(\"The goal of Generative AI is to <mask> new content.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c71a105-fca4-4f03-8268-fae4b5261015",
   "metadata": {},
   "source": [
    "## Experiment 3: Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a798fbda-a93c-48eb-946a-554476e7904f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT - Question Answering\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9fd263d975f42d4823f79d64864ea63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/197 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForQuestionAnswering LOAD REPORT from: bert-base-uncased\n",
      "Key                                        | Status     | \n",
      "-------------------------------------------+------------+-\n",
      "bert.pooler.dense.bias                     | UNEXPECTED | \n",
      "cls.predictions.bias                       | UNEXPECTED | \n",
      "cls.predictions.transform.dense.bias       | UNEXPECTED | \n",
      "bert.pooler.dense.weight                   | UNEXPECTED | \n",
      "cls.predictions.transform.LayerNorm.weight | UNEXPECTED | \n",
      "cls.seq_relationship.bias                  | UNEXPECTED | \n",
      "cls.predictions.transform.LayerNorm.bias   | UNEXPECTED | \n",
      "cls.predictions.transform.dense.weight     | UNEXPECTED | \n",
      "cls.seq_relationship.weight                | UNEXPECTED | \n",
      "qa_outputs.weight                          | MISSING    | \n",
      "qa_outputs.bias                            | MISSING    | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.008222623029723763,\n",
       " 'start': 46,\n",
       " 'end': 60,\n",
       " 'answer': 'hallucinations'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1. BERT\n",
    "\n",
    "print(\"BERT - Question Answering\")\n",
    "\n",
    "bert_qa = pipeline(\n",
    "    \"question-answering\",\n",
    "    model=\"bert-base-uncased\"\n",
    ")\n",
    "\n",
    "bert_qa(\n",
    "    question=\"What are the risks?\",\n",
    "    context=\"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2cb11a42-6ac1-48fd-81de-f14d7b79fcb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RoBERTa - Question Answering\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffa5b38976e64966a0506925d489d082",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/197 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RobertaForQuestionAnswering LOAD REPORT from: roberta-base\n",
      "Key                             | Status     | \n",
      "--------------------------------+------------+-\n",
      "lm_head.dense.bias              | UNEXPECTED | \n",
      "roberta.embeddings.position_ids | UNEXPECTED | \n",
      "lm_head.bias                    | UNEXPECTED | \n",
      "lm_head.layer_norm.bias         | UNEXPECTED | \n",
      "lm_head.layer_norm.weight       | UNEXPECTED | \n",
      "lm_head.dense.weight            | UNEXPECTED | \n",
      "qa_outputs.bias                 | MISSING    | \n",
      "qa_outputs.weight               | MISSING    | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.008536022156476974,\n",
       " 'start': 20,\n",
       " 'end': 81,\n",
       " 'answer': 'significant risks such as hallucinations, bias, and deepfakes'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2. RoBERTa  \n",
    "\n",
    "print(\"RoBERTa - Question Answering\")\n",
    "\n",
    "roberta_qa = pipeline(\n",
    "    \"question-answering\",\n",
    "    model=\"roberta-base\"\n",
    ")\n",
    "\n",
    "roberta_qa(\n",
    "    question=\"What are the risks?\",\n",
    "    context=\"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40e0ed9c-adcc-4756-bdc9-2228d92e2942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BART - Question Answering\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21d6b68d14ce43e2ad15571e814d0e32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BartForQuestionAnswering LOAD REPORT from: facebook/bart-base\n",
      "Key               | Status  | \n",
      "------------------+---------+-\n",
      "qa_outputs.weight | MISSING | \n",
      "qa_outputs.bias   | MISSING | \n",
      "\n",
      "Notes:\n",
      "- MISSING\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.015596938319504261,\n",
       " 'start': 43,\n",
       " 'end': 66,\n",
       " 'answer': 'as hallucinations, bias'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#3. BART\n",
    "\n",
    "print(\"BART - Question Answering\")\n",
    "\n",
    "bart_qa = pipeline(\n",
    "    \"question-answering\",\n",
    "    model=\"facebook/bart-base\"\n",
    ")\n",
    "\n",
    "bart_qa(\n",
    "    question=\"What are the risks?\",\n",
    "    context=\"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
